{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d488f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "from utils import load_problems\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ed6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = SimpleNamespace(\n",
    "    model_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    tokenizer_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    intervention_vector_path=\"tpv_model/tpv_linear_weights.npy\",\n",
    "    output_generations_dir=\"llama_intervention_responses\",\n",
    "    dataset=\"math500\",\n",
    "    task_name=\"math500\",\n",
    "    problem_start_idx=30,\n",
    "    problem_end_idx=31,\n",
    "    alpha=100.0,\n",
    "    max_new_tokens=2048,\n",
    "    device=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    ")\n",
    "if args.tokenizer_name_or_path is None:\n",
    "    args.tokenizer_name_or_path = args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Loading tokenizer from: {args.tokenizer_name_or_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    logging.info(\"Set pad_token to eos_token.\")\n",
    "logging.info(f\"Loading model from: {args.model_name_or_path}\")\n",
    "model_dtype = getattr(torch, args.torch_dtype)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    device_map=args.device if args.device != \"auto\" else \"auto\",\n",
    "    torch_dtype=model_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Determine the device the model ended up on\n",
    "effective_device = model.device\n",
    "logging.info(\n",
    "    f\"Model loaded. Effective device for intervention vector: {effective_device}\"\n",
    ")\n",
    "logging.info(\"Preparing intervention vector...\")\n",
    "hidden_dim = model.config.hidden_size\n",
    "logging.info(f\"Model hidden dimension: {hidden_dim}\")\n",
    "intervention_vector = torch.Tensor(np.load(args.intervention_vector_path)).to(\n",
    "    effective_device\n",
    ")\n",
    "# Reshape to a 1D vector of size hidden_dim\n",
    "intervention_vector = intervention_vector.squeeze()\n",
    "if intervention_vector.ndim != 1:\n",
    "    raise ValueError(\n",
    "        f\"Intervention vector must be 1D after squeezing, but got shape {intervention_vector.shape}. \"\n",
    "        f\"Original path: {args.intervention_vector_path}\"\n",
    "    )\n",
    "if intervention_vector.shape[0] != hidden_dim:\n",
    "    raise ValueError(\n",
    "        f\"Intervention vector dimension ({intervention_vector.shape[0]}) does not match model hidden dimension ({hidden_dim}).\"\n",
    "    )\n",
    "logging.info(\n",
    "    f\"Intervention vector loaded and reshaped to: {intervention_vector.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cedb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_intervention(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=1,\n",
    "    intervention_vector=None,\n",
    "    intervention_scale=1.0\n",
    "):\n",
    "    \"\"\"Generates tokens with intervention by hooking into the generation process.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    hook_handles = []\n",
    "\n",
    "    def forward_hook(module, input_args, output):\n",
    "        # input_args is a tuple, output can be a tensor or a tuple\n",
    "        # We are interested in modifying the first element of the output if it's a tuple (common for hidden states)\n",
    "        # or the output itself if it's a tensor.\n",
    "\n",
    "        original_output = output\n",
    "        hidden_states_to_modify = None\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states_to_modify = output[0]\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            hidden_states_to_modify = output\n",
    "        else:\n",
    "            logging.warning(f\"Unexpected output type from hooked layer: {type(output)}\")\n",
    "            return output\n",
    "\n",
    "        if hidden_states_to_modify is None or not isinstance(\n",
    "            hidden_states_to_modify, torch.Tensor\n",
    "        ):\n",
    "            logging.warning(\n",
    "                \"Hooked layer output does not contain a tensor of hidden states as expected.\"\n",
    "            )\n",
    "            return output\n",
    "\n",
    "        modified_states = hidden_states_to_modify.clone()\n",
    "\n",
    "        # Reshape intervention vector to [1, 1, hidden_dim] to broadcast correctly\n",
    "        reshaped_vector = intervention_vector.view(1, 1, -1).to(\n",
    "            hidden_states_to_modify.device\n",
    "        )\n",
    "\n",
    "        # Apply intervention to the last token position's hidden state\n",
    "        modified_states[:, -1:, :] = modified_states[:, -1:, :] + (\n",
    "            reshaped_vector * intervention_scale\n",
    "        )\n",
    "\n",
    "        if isinstance(original_output, tuple):\n",
    "            return (modified_states,) + original_output[1:]\n",
    "        else:\n",
    "            return modified_states\n",
    "\n",
    "    # Always use model.model.norm as the intervention target\n",
    "    target_intervention_module = model.model.norm\n",
    "\n",
    "    if intervention_vector is not None:\n",
    "        if not hasattr(target_intervention_module, \"register_forward_hook\"):\n",
    "            raise ValueError(\n",
    "                f\"The module model.model.norm does not have 'register_forward_hook' method.\"\n",
    "            )\n",
    "        hook_handles.append(\n",
    "            target_intervention_module.register_forward_hook(forward_hook)\n",
    "        )\n",
    "        logging.debug(\n",
    "            f\"Intervention hook registered on model.model.norm ({type(target_intervention_module)}).\"\n",
    "        )\n",
    "        logging.debug(f\"Intervention vector shape: {intervention_vector.shape}\")\n",
    "        logging.debug(f\"Intervention scale: {intervention_scale}\")\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():  # Standard practice for inference\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,  # Greedy decoding for reproducibility\n",
    "                num_beams=1,\n",
    "                use_cache=True,  # Use KV cache for efficiency\n",
    "                pad_token_id=(\n",
    "                    tokenizer.pad_token_id\n",
    "                    if tokenizer.pad_token_id is not None\n",
    "                    else tokenizer.eos_token_id\n",
    "                ),\n",
    "            )\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    finally:\n",
    "        for handle in hook_handles:\n",
    "            handle.remove()\n",
    "        if hook_handles:\n",
    "            logging.debug(\"All intervention hooks removed.\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using only a single alpha value as requested\n",
    "intervention_scale = args.alpha\n",
    "logging.info(f\"Using intervention scale (alpha): {intervention_scale}\")\n",
    "# Construct the full path for output generations\n",
    "model_folder_name = args.model_name_or_path.split(\"/\")[-1]\n",
    "generations_base_dir = os.path.join(\n",
    "    args.output_generations_dir, model_folder_name, args.task_name, \"intervention\"\n",
    ")\n",
    "problems = load_problems(args.dataset, args.problem_start_idx, args.problem_end_idx)\n",
    "\n",
    "for i, problem in enumerate(problems):\n",
    "    problem_num = i\n",
    "    logging.info(f\"Processing problem {problem_num}...\")\n",
    "    problem_output_dir = os.path.join(\n",
    "        generations_base_dir, f\"problem_{problem_num}\"\n",
    "    )\n",
    "    os.makedirs(problem_output_dir, exist_ok=True)\n",
    "    prompt = (\n",
    "        problem\n",
    "        + \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\\n<think>\\n\"\n",
    "    )\n",
    "    # Generate with single intervention scale\n",
    "    output_with_intervention = generate_with_intervention(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        intervention_vector=intervention_vector,\n",
    "        intervention_scale=intervention_scale,\n",
    "    )\n",
    "    output_file_path = os.path.join(\n",
    "        problem_output_dir, f\"response_alpha_{args.alpha}.txt\"\n",
    "    )\n",
    "    try:\n",
    "        with open(output_file_path, \"w\") as f:\n",
    "            f.write(output_with_intervention)\n",
    "        logging.info(\n",
    "            f\"Saved response for problem {problem_num}, alpha {args.alpha} to {output_file_path}\"\n",
    "        )\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to write output file {output_file_path}: {e}\")\n",
    "\n",
    "logging.info(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
